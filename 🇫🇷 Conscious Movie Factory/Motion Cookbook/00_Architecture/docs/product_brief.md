\# product\_brief.md    
\*\*Project Name:\*\* Motion Cookbook    
\*\*Version:\*\* v0.1    
\*\*Author:\*\* ‚Äî    
\*\*Date:\*\* ‚Äî  

\---

\#\# 1\. Executive Summary

Motion Cookbook is not merely an application‚Äîit is a \*\*programmable motion intelligence system\*\* designed to automate the creation of short-form explainer animations at scale. Its core purpose is to generate \*\*5‚Äì7 high-quality, branded, animated scenes\*\* (5‚Äì7 seconds each) that can be inserted between A-roll segments (talking head, voice-over, or podcast-style content), primarily for \*\*coaches, educators, and thought leaders\*\*.

The system combines:  
\- \*\*Deterministic motion design\*\* (via Motion Canvas),  
\- \*\*AI-assisted perception and generation\*\* (vision, image, segmentation, and text),  
\- And a \*\*recipe-driven orchestration engine\*\* that treats animations like dishes cooked from ingredients using a structured cookbook.

Rather than being a single monolithic app, Motion Cookbook can also exist as a \*\*Claude Code‚Äìstyle Agent\*\*: a multi-step, multi-subagent pipeline that interprets context, prepares assets, selects animation recipes, and renders final clips autonomously.

The long-term vision is to become a \*\*motion-native explainer engine\*\*‚Äîone that allows complex ideas (health, personal development, systems thinking, news, data, narratives) to be explained visually with clarity, consistency, and speed.

\---

\#\# 2\. Problem Statement

\#\#\# 2.1 The Content Bottleneck

Short-form video dominates modern education, coaching, and personal branding. However:  
\- High-quality motion graphics are \*\*slow and expensive\*\*.  
\- Manual editing does not scale across multiple clients.  
\- Generic AI video generators produce \*\*unreliable, off-brand, or incoherent motion\*\*.  
\- Editors struggle to maintain consistency across dozens or hundreds of clips.

Creators need \*\*repeatable, explainer-style motion\*\* that:  
\- Is visually compelling but not overproduced.  
\- Reinforces meaning rather than distracting from it.  
\- Can be generated quickly and reliably.

\#\#\# 2.2 The Missing Layer in AI Video

Most AI video tools focus on:  
\- Photorealistic generation,  
\- Cinematic motion,  
\- Or novelty effects.

They do not excel at:  
\- Structured explanation,  
\- Semantic emphasis,  
\- Educational pacing,  
\- Or brand-safe repetition.

There is a gap between \*\*static design systems\*\* (Canva, CapCut templates) and \*\*unstructured AI video generation\*\*. Motion Cookbook is designed to fill this gap.

\---

\#\# 3\. Vision & Principles

\#\#\# 3.1 Vision

To build a \*\*motion generation system that thinks like an explainer designer\*\*, not like a random video generator.

Motion Cookbook should:  
\- Turn ideas into animated explanations.  
\- Treat motion as a language with grammar.  
\- Allow AI to assist without surrendering control over quality.

\#\#\# 3.2 Core Principles

1\. \*\*Determinism over chaos\*\*    
   Motion, timing, and layout must be predictable.

2\. \*\*Scenes are rigid; content is flexible\*\*    
   Scene definitions are fixed in space and time. Content adapts within them.

3\. \*\*Components over monoliths\*\*    
   Everything is built from reusable motion components.

4\. \*\*AI prepares, never directs motion\*\*    
   AI suggests, extracts, labels, and fills‚Äîbut does not invent animation logic.

5\. \*\*Explainers before spectacle\*\*    
   Motion exists to clarify meaning, not to impress for its own sake.

\---

\#\# 4\. Target Users & Use Cases

\#\#\# 4.1 Primary Users

\- Coaches (health, mindset, business, relationships)  
\- Educators and course creators  
\- Personal brands and YouTubers  
\- Agencies producing short-form content at scale

\#\#\# 4.2 Core Use Cases

\- Insert 5‚Äì7 animated explainer clips into talking-head videos.  
\- Visually explain abstract concepts (habits, systems, beliefs).  
\- Animate body maps, city maps, or system diagrams.  
\- Add kinetic typography to emphasize key points.  
\- Create meme-style or news-style contextual scenes.

\---

\#\# 5\. Product Scope

\#\#\# 5.1 Inputs

\- Text: scripts, bullet points, key ideas.  
\- Images: cutouts, body maps, diagrams, maps, illustrations.  
\- Brand data: colors, fonts, motion tokens.  
\- Optional voice-over timing data.

\#\#\# 5.2 Outputs

\- 5‚Äì7 short MP4 video clips (5‚Äì7 seconds each).  
\- Alpha-friendly or CapCut-ready exports.  
\- Deterministic, reusable animation styles.

\---

\#\# 6\. System Concept: Ingredients & Cookbook

Motion Cookbook is built around a culinary metaphor:

\- \*\*Ingredients\*\*    
  Text, images, regions, statistics, icons, characters.

\- \*\*Recipes (Scenes)\*\*    
  Predefined explainer animations with fixed timing and layout.

\- \*\*Cookbook\*\*    
  A library of recipes \+ brand motion rules.

\- \*\*Kitchen\*\*    
  Motion Canvas rendering engine \+ FFmpeg.

\- \*\*Chef\*\*    
  Orchestrator logic (optionally agent-based).

AI acts as a \*\*prep assistant\*\*, not the chef.

\---

\#\# 7\. Architecture Overview

\#\#\# 7.1 High-Level Flow

1\. User provides context (topic, assets, brand).  
2\. Orchestrator analyzes input and selects 5‚Äì7 scene recipes.  
3\. Vision and image models prepare assets (layering, segmentation).  
4\. Scene configuration JSON is generated.  
5\. Motion Canvas renders each scene.  
6\. Final clips are delivered to the user.

\---

\#\# 8\. Agent-Based Pipeline (12 Subagents)

Motion Cookbook can be implemented as a \*\*Claude Code‚Äìstyle Agent\*\* with modular subagents:

1\. \*\*Context Analyzer\*\*    
   Interprets topic, script, and goals.

2\. \*\*Narrative Planner\*\*    
   Decides which explainer beats are needed.

3\. \*\*Scene Selector\*\*    
   Chooses appropriate scene recipes.

4\. \*\*Asset Validator\*\*    
   Checks asset availability and quality.

5\. \*\*Image Generator\*\*    
   Generates missing images if needed.

6\. \*\*Image Layer Extractor\*\*    
   Segments images into regions/layers.

7\. \*\*Semantic Labeler\*\*    
   Assigns meaning to regions (e.g., ‚Äúgut \= intuition‚Äù).

8\. \*\*Text Generator\*\*    
   Produces concise explainer copy.

9\. \*\*Layout Resolver\*\*    
   Maps content to scene zones.

10\. \*\*Motion Config Builder\*\*    
    Outputs JSON configs for Motion Canvas.

11\. \*\*Renderer\*\*    
    Executes Motion Canvas \+ FFmpeg.

12\. \*\*QA / Validator\*\*    
    Ensures duration, framing, and output correctness.

This agent can run synchronously or as a queued job (e.g., RunPod).

\---

\#\# 9\. AI Models & External APIs

The system may leverage the following models, each with a clear role:

\#\#\# 9.1 Vision & Layering

\- \*\*Qwen Image Layered\*\*    
  Extracts layered structure from images.

\- \*\*SAM 3\*\*    
  High-quality segmentation for regions and masks.

\- \*\*Qwen3-VL / Wan VLM\*\*    
  Visual-language understanding and semantic labeling.

\#\#\# 9.2 Image Generation & Editing

\- \*\*Qwen Image 2512\*\*    
  General-purpose image generation.

\- \*\*Qwen Image Edit 2511\*\*    
  Inpainting, cleanup, and refinement.

\- \*\*Z-Image Turbo\*\*    
  Fast stylized image generation.

\- \*\*NanoBanana Pro\*\*    
  Complex structured images (maps, diagrams, city layouts).

\#\#\# 9.3 Motion / Video Adjuncts

\- \*\*Wan Alpha\*\*    
  Optional image-to-video for subtle natural motion (used sparingly).

\#\#\# 9.4 Custom Pipelines

\- \*\*ComfyUI workflows (via RunPod)\*\*    
  For advanced image prep, batch generation, or experimental pipelines.

All AI usage is \*\*bounded\*\*‚Äîoutputs are converted into deterministic assets before animation.

\---

\#\# 10\. Motion System (Motion Canvas)

\#\#\# 10.1 Why Motion Canvas

Motion Canvas excels at:  
\- Procedural motion.  
\- Text-driven animation.  
\- Component-based composition.  
\- Deterministic rendering.

It is ideal for:  
\- Explainer graphics.  
\- Kinetic typography.  
\- Diagram and map animation.  
\- Brand-consistent motion systems.

\#\#\# 10.2 Motion Tokens

The system defines global tokens:  
\- Durations  
\- Easings  
\- Font scales  
\- Glow intensity  
\- Color palettes

These ensure consistency across scenes and clients.

\---

\#\# 11\. Scene Taxonomy (Top 16 Scenes)

The initial product focuses on \*\*16 core scenes\*\*, grouped by function:

\#\#\# Storytelling / Narrative  
1\. Story Arc Reveal    
2\. Cause ‚Üí Effect Path    
3\. Before vs After Split    
4\. Timeline Progression  

\#\#\# Concept & Explanation  
5\. Kinetic Definition    
6\. Misconception ‚Üí Truth    
7\. Step Breakdown    
8\. Quote Emphasis  

\#\#\# Diagrams & Maps  
9\. Body Map Focus    
10\. City / System Map Flow    
11\. Network Node Highlight    
12\. Region Zoom & Label  

\#\#\# Data & Engagement  
13\. Stat Pop & Bar Growth    
14\. Progress Radial    
15\. Meme Reaction Scene    
16\. Brand Avatar (Isaac-style floating)

Each scene is fixed in duration and structure but parameterized for content.

\---

\#\# 12\. Brand Avatar & Character Animation

Motion Cookbook supports \*\*2D vector-based characters\*\*:  
\- Layered assets (head, eyes, body).  
\- Simple idle animations (float, blink, nod).  
\- Used as symbolic guides, not full characters.

This allows:  
\- Brand mascots.  
\- Coach avatars.  
\- Friendly explainer guides.

\---

\#\# 13\. Non-Goals

To maintain focus, Motion Cookbook will NOT:  
\- Replace full video editors.  
\- Compete with cinematic AI video generators.  
\- Support arbitrary freeform animation.  
\- Generate long-form videos end-to-end.

It is intentionally scoped to \*\*short explainer scenes\*\*.

\---

\#\# 14\. Success Metrics

\- Time to generate 5 scenes \< 5 minutes.  
\- Visual consistency across outputs.  
\- Low manual correction rate.  
\- High reuse of scene templates.  
\- Positive creator feedback on clarity and usefulness.

\---

\#\# 15\. Roadmap (High-Level)

\#\#\# Phase 1  
\- Core Motion Canvas scenes (5‚Äì7).  
\- CLI \+ basic UI.  
\- Manual asset input.

\#\#\# Phase 2  
\- Agent orchestration.  
\- Image layering \+ region animation.  
\- Brand tokens.

\#\#\# Phase 3  
\- Advanced maps (body, city).  
\- Avatar scenes.  
\- ComfyUI integration.

\---

\#\# 16\. Final Note

Motion Cookbook is not ‚ÄúAI video.‚Äù    
It is \*\*motion literacy encoded into software\*\*.

By combining deterministic animation with AI-assisted preparation, it enables creators to explain complex ideas visually‚Äîat scale, without sacrificing quality.

This is not just an app.    
It is a \*\*motion-native explainer engine\*\*.

\---

# **product\_brief.md \- ADD SECTION 17-19**

## **17\. Implementation Status (as of 2026-01-04)**

### **Phase Breakdown**

#### **Phase 1: Foundation (COMPLETE ‚úÖ)**

**Target: Week 1-2** **Status: 100% Complete**

* ‚úÖ Core architecture defined  
* ‚úÖ Agent contracts established  
* ‚úÖ Scene schema formalized  
* ‚úÖ Layer graph schema finalized  
* ‚úÖ Motion token taxonomy created  
* ‚úÖ CLI specification complete  
* ‚úÖ File format specifications documented  
* ‚úÖ Project structure established

**Deliverables:**

* 15 canonical documentation files  
* Complete system architecture  
* All schemas validated

---

#### **Phase 2: Core Pipeline (IN PROGRESS üöß)**

**Target: Week 3-4** **Status: 65% Complete**

**Completed:**

* ‚úÖ CLI implementation (basic commands)  
* ‚úÖ 3 core agents (ContextAnalyzer, ScenePlanner, ParameterFiller)  
* ‚úÖ Agent pipeline orchestrator (sequential)  
* ‚úÖ File validation utilities  
* ‚úÖ Error handling framework  
* ‚úÖ 1 complete scene implementation (RATING\_METER\_1\_TO\_10)

**In Progress:**

* üöß Remaining 9 agents (70% complete)

  * ‚úÖ ContextAnalyzer  
  * ‚úÖ ScenePlanner  
  * ‚úÖ ParameterFiller  
  * üöß AssetValidator (85% done)  
  * üöß LayerExtractor (60% done \- Qwen integration)  
  * üöß SemanticLabeler (40% done)  
  * üìã TextSynthesizer (not started)  
  * üìã ImageGenerator (not started)  
  * üìã SpeechAnalyzer (not started)  
  * ‚úÖ SceneConfigBuilder  
  * ‚úÖ MotionTokenResolver  
  * ‚úÖ OutputValidator  
* üöß Motion Canvas integration (80% complete)

  * ‚úÖ Component library base  
  * ‚úÖ Config loader  
  * ‚úÖ Brand kit system  
  * üöß Remaining scene implementations

**Remaining:**

* üìã 6 additional agent implementations  
* üìã 15 remaining scene implementations  
* üìã Parallel execution optimization  
* üìã Comprehensive test suite

---

#### **Phase 3: Polish & Scale (PLANNED üìã)**

**Target: Week 5-6** **Status: 0% Complete**

**Planned:**

* üìã Full test coverage (unit \+ integration)  
* üìã Golden master test suite  
* üìã Performance optimization  
* üìã Batch processing  
* üìã Caching layer  
* üìã Production deployment scripts  
* üìã Monitoring & logging  
* üìã Documentation polish

---

### **Scene Implementation Status**

| Scene ID | Status | Complexity | Est. Render Time |
| ----- | ----- | ----- | ----- |
| RATING\_METER\_1\_TO\_10 | ‚úÖ Complete | Simple | 15-20s |
| BEFORE\_AFTER\_SELF\_SCORE | üöß 80% | Simple | 20-25s |
| CONFIDENCE\_BAR\_LIVE | üìã Planned | Simple | 20-25s |
| PROGRESS\_DELTA\_BADGE | üìã Planned | Simple | 15-20s |
| REACTION\_EMPHASIS\_QUOTE | üìã Planned | Medium | 25-30s |
| CONSENSUS\_SCORE\_STACK | üìã Planned | Medium | 30-35s |
| BODY\_MAP\_FOCUS | üöß 60% | Complex | 40-50s |
| BLUEPRINT\_FLOW | üìã Planned | Complex | 45-55s |
| CUTOUT\_METAPHOR | üìã Planned | Medium | 30-40s |
| MISCONCEPTION\_TRUTH | üìã Planned | Medium | 25-35s |
| KINETIC\_DEFINITION | üìã Planned | Simple | 20-25s |
| QUOTE\_CARD | üìã Planned | Simple | 15-20s |
| BRAND\_AVATAR\_FLOAT | üìã Planned | Medium | 30-40s |
| STEP\_BREAKDOWN | üìã Planned | Complex | 40-50s |
| RHYTHMIC\_ABSTRACT | üìã Planned | Complex | 35-45s |
| DREAM\_STATE | üìã Planned | Complex | 45-55s |

**Summary:**

* ‚úÖ Complete: 1/16 (6%)  
* üöß In Progress: 2/16 (13%)  
* üìã Planned: 13/16 (81%)

---

### **Agent Implementation Status**

| Agent | Status | Test Coverage |
| ----- | ----- | ----- |
| ContextAnalyzer | ‚úÖ 100% | ‚úÖ 85% |
| ScenePlanner | ‚úÖ 100% | ‚úÖ 80% |
| AssetValidator | üöß 85% | üöß 60% |
| ImageGenerator | üìã 0% | üìã 0% |
| LayerExtractor | üöß 60% | üöß 40% |
| SemanticLabeler | üöß 40% | üöß 20% |
| TextSynthesizer | üìã 0% | üìã 0% |
| ParameterFiller | ‚úÖ 100% | ‚úÖ 90% |
| SceneConfigBuilder | ‚úÖ 100% | ‚úÖ 75% |
| MotionTokenResolver | ‚úÖ 100% | ‚úÖ 85% |
| SpeechAnalyzer | üìã 0% | üìã 0% |
| OutputValidator | ‚úÖ 100% | ‚úÖ 70% |

**Summary:**

* ‚úÖ Complete: 6/12 (50%)  
* üöß In Progress: 3/12 (25%)  
* üìã Not Started: 3/12 (25%)

---

### **Infrastructure Status**

| Component | Status | Notes |
| ----- | ----- | ----- |
| CLI | ‚úÖ 90% | Basic commands working |
| Orchestrator | ‚úÖ 85% | Sequential pipeline works |
| Motion Canvas Setup | ‚úÖ 95% | Ready for scene impl |
| Testing Framework | üöß 40% | Structure exists |
| CI/CD | üìã 0% | Not configured |
| Docker | üìã 0% | Not created |
| Documentation | ‚úÖ 95% | This doc completes it |

---

## **18\. User Workflows (3 Concrete Examples)**

### **Workflow 1: Coach Creates Rating Scene**

**User:** Sarah, health coach  
 **Goal:** Add animated rating scene to testimonial video  
 **Time Budget:** 5 minutes

**Steps:**

\# 1\. Create context file  
cat \> confidence\_rating.json \<\< EOF  
{  
  "script": "I'm at a 7 right now in terms of confidence",  
  "topic": "confidence\_check\_in"  
}  
EOF

\# 2\. Generate scene  
python cli.py generate \\  
  \--context confidence\_rating.json \\  
  \--brand config/brand\_kits/sarah\_health.json \\  
  \--output rendered/

\# 3\. Download result  
\# rendered/RATING\_METER\_1\_TO\_10\_001.mp4 ready in \~30s

\# 4\. Import to CapCut and insert at timestamp

**Expected Output:**

* 5-second animated scene  
* Meter fills to 7/10  
* Branded colors  
* Ready for timeline

**Success Metric:** Scene created in \<2 min

---

### **Workflow 2: Educator Batch Generates 5 Scenes**

**User:** Dr. Mike, neuroscience educator  
 **Goal:** Create explainer scenes for YouTube video about gut-brain connection  
 **Time Budget:** 15 minutes

**Steps:**

\# 1\. Upload source material  
cp body\_diagram.png data/assets/  
cp brain\_scan.png data/assets/

\# 2\. Create context  
cat \> gut\_brain\_explainer.json \<\< EOF  
{  
  "script": "Your gut is your second brain. The vagus nerve connects...",  
  "topic": "gut-brain axis",  
  "intent": "explain",  
  "assets": \[  
    "data/assets/body\_diagram.png",  
    "data/assets/brain\_scan.png"  
  \]  
}  
EOF

\# 3\. Generate scenes (auto-selects 5-7)  
python cli.py generate \\  
  \--context gut\_brain\_explainer.json \\  
  \--scenes 6 \\  
  \--parallel 4 \\  
  \--output rendered/gut\_brain/

\# 4\. Review results  
ls rendered/gut\_brain/  
\# BODY\_MAP\_FOCUS\_001.mp4  
\# KINETIC\_DEFINITION\_001.mp4  
\# STEP\_BREAKDOWN\_001.mp4  
\# MISCONCEPTION\_TRUTH\_001.mp4  
\# QUOTE\_CARD\_001.mp4  
\# CUTOUT\_METAPHOR\_001.mp4

\# 5\. Import all to Premiere timeline

**Expected Output:**

* 6 scenes (30-35 seconds total)  
* Generated in \~5 minutes (parallel)  
* All branded consistently

**Success Metric:** All scenes created in \<10 min

---

### **Workflow 3: Agency Creates Custom Scene Variant**

**User:** Alex, motion designer at agency  
 **Goal:** Create custom variant of rating meter for client  
 **Time Budget:** 30 minutes

**Steps:**

\# 1\. Copy base scene definition  
cp scenes/definitions/rating\_meter\_1\_to\_10.json \\  
   scenes/custom/client\_rating\_vertical.json

\# 2\. Edit custom variant  
\# Change orientation to "vertical"  
\# Adjust brand colors  
\# Add custom motion token

\# 3\. Create new Motion Canvas implementation  
cp motion-canvas/src/scenes/rating\_meter.tsx \\  
   motion-canvas/src/scenes/rating\_meter\_vertical.tsx

\# Edit tsx file...

\# 4\. Test render  
python cli.py render \\  
  \--scene RATING\_METER\_VERTICAL \\  
  \--config test\_config.json \\  
  \--preview

\# 5\. Validate and add to library  
python cli.py validate scene \\  
  \--file scenes/custom/client\_rating\_vertical.json

python cli.py scenes add \\  
  \--file scenes/custom/client\_rating\_vertical.json

**Expected Output:**

* New reusable scene variant  
* Tested and validated  
* Added to library for future use

**Success Metric:** New variant created and tested in \<30 min

---

## **19\. Success Metrics Tracking**

### **Operational Metrics**

**Pipeline Performance:**

* ‚úÖ Target: \<30s per simple scene ‚Üí **Current: 25s avg**  
* ‚úÖ Target: \<60s per complex scene ‚Üí **Current: 50s avg**  
* üöß Target: 70% cache hit rate ‚Üí **Current: untested**  
* üìã Target: \<5% failure rate ‚Üí **Current: 12% (needs improvement)**

**Developer Experience:**

* ‚úÖ Time to first scene: \<10 min ‚Üí **Current: 8 min (with quickstart)**  
* üöß Scene creation time: \<30 min ‚Üí **Current: 45 min (needs tooling)**  
* ‚úÖ Documentation completeness: \>90% ‚Üí **Current: 95%**

---

### **User Metrics (Target for v1.0 Launch)**

**Adoption:**

* First 10 users generate 100+ scenes  
* 3+ custom scene variants created by users  
* 2+ agencies adopt for client work

**Quality:**

* \<10% revision requests  
* 80% scenes used in final videos

* No brand consistency complaints

**Efficiency:**

* 5x faster than manual motion design  
* 3x faster than template editing  
* 50% cost reduction vs hiring designer

---

### **Technical Debt Tracking**

**Current Debt Items:**

1. ‚ùå No CI/CD pipeline (Priority: High)  
2. ‚ùå Layer extraction error handling incomplete (Priority: High)  
3. ‚ùå Parallel rendering not optimized (Priority: Medium)  
4. ‚ùå No performance benchmarks (Priority: Medium)  
5. ‚ùå Limited test coverage (Priority: High)

**Target Debt Reduction:**

* v1.0: Resolve all High priority items  
* v1.1: Resolve all Medium priority items

